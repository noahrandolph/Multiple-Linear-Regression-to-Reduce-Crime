---
title: "Lab 4: Reducing Crime"
author: "Asha Anju, Anamika Sinha, Noah Randolph"
date: "8/14/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

...

## Load Data & Initial Analysis

We first load the data and note any anomalies, such as missing values, top-coded or bottom-coded variables, or not-applicable values.

```{r load data}
library(sandwich)
library(lmtest)
library(car)
library(stargazer)

setwd("~/Documents/W203_Statistics/Labs/Lab_4/Lab4Repository")
CrDat = read.csv("crime_v2.csv")
summary(CrDat)
attach(CrDat)
```

There are no missing values and the data in each variable make sense given the variable types, except for values in _probsen_ and _probconv_, which both show proportions that are greater than 1. These proportions are replaced with NA values below, so that the remaining row of values associated with each county are still usable for models not involving these variables.

```{r}
# tail(sort(probsen))
# CrDat[probsen > 1, 'probsen'] = NA
# 
# tail(sort(probconv), 11)
# CrDat[probconv > 1, 'probconv'] = NA
```

## Model Building and Exploratory Data Analysis

We first look at the response variable, _crime_, representing the number of each counties' crimes committed per person.

```{r}
hist(crime, breaks = 20, main = "County Crime Rates", xlab = "crimes committed per person")
```

The histogram above shows an approximatley normal distribution with a positive skew, likely due to the fact that crime rates cannot be less than zero. No outliars are present. 


Several variables standout as good candidates for an initial model attempting to understand the determinants of crime. These include the proportion of crimes resulting in arrests, _probarr_, the average prison sentence length, _avgsen_, police per capita, _police_, people per square mile, _density_, average wages (which are spread across several variables), and the proportion of males between ages 15 and 24, _ymale_. 

The proportion of crimes resulting in arrests, _probarr_, should negatively influence crime rate, intuitively, in the same way that increased ticketing on highways should reduce speeding. The histogram and number of observations are shown below.

```{r}
length(probarr)

hist(probarr, breaks = 16, main = "Proportion of County Crimes Resulting in Arrests",
     xlab = "Proportion of Crimes Resulting in Arrests")
```

The histogram above shows an approximately normal distribution. With 90 observations, the central limit theorem (CLT) can be relied upon to make an inference about this variable's regression coefficient as it corresponds to the population.

The average prison sentence length, _avgsen_, like _probarr_, should also negatively influence crime rate, as longer sentences increase the consequences to committing a crime.

```{r}
hist(probconv, breaks = 10, main = "Proportion Being Convicted After Arrest",
     xlab = "Proportion")
```


```{r}
length(probsen)

hist(probsen, breaks = 20, main = "Proportion Being Sentenced After Conviction",
     xlab = "Proportion")
```

The prison sentence length distribution, above, is approximately normal with a positive skew, likely due to the fact that the metric is zero-bounded. As with _probarr_, 90 observations is sufficient to rely on the CLT for population inferences.

Police per capita, _police_, should have negative influence on county crime rates, since their presence is generally thought of as a deterence.

```{r}
length(police)

hist(police, breaks = 20, main = "Police Per Capita",
     xlab = "police per capita")
```


There is a large spike at 0.001 to 0.002 police per capita and short tails on either side, with a positive skew, including an outliar at 0.009 to 0.010 police per capita. We investigate that observation further below.

```{r}
CrDat[police > 0.005, ]
```

Noting that it is the same county which had values greater than 1 for _probsen_ and _probconv_ (which are now coded _NA_), the rest of county data does not indicate anything to be invalid. We note that the average sentence length of this county is maximum of all the counties, as seen in the initial summary, and the percent minority is the lowest of all the counties.

The variable describing people per square mile, _density_, is a more detailed indicator of urban versus non-urban environments than the boolean varianble, _urban_. Density should also influence crime rate, either positively or negatively, since 'haves' and 'havenots' become closer together at higher densities, yet higher density areas also result in more witnesses to deter face-to-face crime.

```{r}
length(density)

hist(density, breaks = 20, main = "Density",
     xlab = "people per square mile")
```

Density's distribution has a strong positive skew, due to the zero bound. Despite the skew, the CLT can still be relied upon for population inferences, as the number of observations, 90, is sufficient.

Average wages, _avgwage_ is a variable created below by concatenating each wage variable in the dataset together. In this way, average wages for each county can be modeled against crime.

```{r}
avgwage = (wagecon + wagetuc + wagetrd + wagefir +
        wageser + wagemfg + wagefed + wagesta + 
                wageloc)/9

length(avgwage)
hist(avgwage, breaks = 20, main = "Average Wage", xlab = "Wage")
```

Average wage also has an approximately normal distribution, with a positive skew. 

```{r}
hist(tax, breaks = 20)
```


```{r}
hist(ymale, breaks = 20)
```

```{r}
hist(pctmin, breaks = 20)
```

```{r}
scatterplotMatrix(~ log(ymale) + log(police) + pctmin +
                          density)
```

```{r}
scatterplotMatrix(~ log(police) + probarr + probconv +
                          probsen)
```

```{r}
plot(jitter(urban), jitter(west))
```

```{r}
plot(jitter(urban), jitter(central))
```

#### Model 1

```{r}
scaledcrime = crime*1000
scaledpolice = police*1000
pctarr = probarr*100
pctconv = probconv*100
pctsen = probsen*100

model1 = lm(scaledcrime ~ pctsen + pctconv + density)
AIC1 = AIC(model1)
plot(model1)

vif(model1)

# hist(model3$residuals, breaks = 50)
# shapiro.test(model3$residuals)
# bptest(model3)
```

#### Model 2

```{r}
model2 = lm(scaledcrime ~ scaledpolice + pctsen + pctconv + density + pctmin)
AIC2 = AIC(model2)
plot(model2)
vif(model2)
```

#### Model 3

```{r}
model3 = lm(scaledcrime ~ scaledpolice + pctsen + pctconv + density + pctmin +
                    pctarr + ymale + avgsen +  mix + avgwage + central + west +
                    tax)
AIC3 = AIC(model3)
plot(model3)
vif(model3)
```



```{r}
# Adjust standard errors
cov1 = vcovHC(model1)
robust_se1 = sqrt(diag(cov1))
cov2 = vcovHC(model2)
robust_se2 = sqrt(diag(cov2))
cov3 = vcovHC(model3)
robust_se3 = sqrt(diag(cov3))

# Adjust F statistic 
wald_results <- waldtest(model1, vcov = cov1)

stargazer(model1, model2, model3, type = 'text', intercept.bottom = FALSE,
          se        = list(robust_se1, robust_se2, robust_se3),
          omit.stat = "f",
          add.lines = list(c("AIC", round(AIC1, 2), round(AIC2, 2), 
                             round(AIC3, 2))))
```

